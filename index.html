<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Zhicheng  Zhang(张知诚)</title>
    <meta name="author" content="Zhicheng  Zhang(张知诚)">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://zzcheng.top/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Zhicheng</span>  Zhang(张知诚)
          </h1>
          <p class="desc">Ph.D. student.  <a href="https://cv.nankai.edu.cn/" rel="external nofollow noopener" target="_blank">CV Lab</a>.   <a href="https://cc.nankai.edu.cn/" rel="external nofollow noopener" target="_blank">Nankai University</a>.   gloryzzc6@sina.com.</p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile.png" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="profile.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

            <div class="address">
              <p>38 Tongyan Road, Jinnan</p> <p>Tianjin, China</p>

            </div>
          </div>

          <div class="clearfix">
            <p>I’m a second year PhD student from <a href="https://cc.nankai.edu.cn/" rel="external nofollow noopener" target="_blank">School of CS</a>, <a href="https://www.nankai.edu.cn/" rel="external nofollow noopener" target="_blank">Nankai University</a>. I am very fortunate to be advised by <a href="https://cv.nankai.edu.cn/" rel="external nofollow noopener" target="_blank">Prof. Jufeng Yang</a> of Computer Vision Lab from Nankai University.</p>

<p>My research interests include computer vision and deep learning, particularly focusing on <strong>video understanding</strong> and <strong>video-language multimodal learning</strong>. You can find my CV here: <a href="../assets/NKU_zzc_CV.pdf">Zhicheng Zhang’s Curriculum Vitae</a>.</p>

<p>For our works, we provide interesting <a href="/projects/">demos</a> for fun. Some examples are listed and you can also upload yours. Moreover, please feel free to make any suggestions. You can contact with me in following ways:
<a href="mailto:gloryzzc6@sina.com">Email1</a> / <a href="mailto:1120210216@mail.nankai.edu.cn">Email2</a> / <a href="https://github.com/nku-zhichengzhang" rel="external nofollow noopener" target="_blank">Github</a> / <a href="../assets/img/Wechat.png">Wechat</a></p>

<center class="half">
    <img src="../assets/img/Github_Wechat.jpg" width="200">
</center>

          </div>

          <!-- News -->
          <h2><a href="/news/" style="color: inherit;">news</a></h2>          <div class="news">
            <div class="table-responsive" style="max-height: 60vw">
              <table class="table table-sm table-borderless">
              
                <tr>
                  <th scope="row" width="15%">Jun 9, 2023</th>
                  <td>
                    <font color="YellowGreen">[Activity]</font>
I attend the VALSE 2023 conference at Wuxi, China.

                  </td>
                </tr>
                <tr>
                  <th scope="row" width="15%">May 11, 2023</th>
                  <td>
                    <font color="DeepSkyBlue">[Award]</font>
I receive the SK AI Innovation Scholarship from SK

                  </td>
                </tr>
                <tr>
                  <th scope="row" width="15%">Mar 15, 2023</th>
                  <td>
                    <font color="red">[Paper]</font>
One paper for plane segmentation is accepted by <a href="https://cv.nankai.edu.cn/" rel="external nofollow noopener" target="_blank"><b><font color="DarkOrchid">Transactions on Neural Networks and Learning Systems</font></b></a> <strong>(TNNLS 2023)</strong>

                  </td>
                </tr>
                <tr>
                  <th scope="row" width="15%">Feb 28, 2023</th>
                  <td>
                    <font color="red">[Paper]</font>
One paper for video emotion analysis is accepted by <strong>CVPR 2023</strong>

                  </td>
                </tr>
                <tr>
                  <th scope="row" width="15%">Aug 22, 2022</th>
                  <td>
                    <font color="YellowGreen">[Activity]</font>
I attend the VALSE 2022 conference and serve as a volunteer. Welcome to Tianjin, China.

                  </td>
                </tr>
                <tr>
                  <th scope="row" width="15%">Jul 3, 2022</th>
                  <td>
                    <font color="red">[Paper]</font>
One paper for temporal sentiment localization is accepted by <strong>ACM Multimedia 2022</strong>

                  </td>
                </tr>
                <tr>
                  <th scope="row" width="15%">Oct 8, 2021</th>
                  <td>
                    <font color="YellowGreen">[Activity]</font>
I attend the VALSE 2021 conference at Hangzhou, China

                  </td>
                </tr>
                <tr>
                  <th scope="row" width="15%">Sep 1, 2021</th>
                  <td>
                    <font color="orange">[Study]</font>
I start my Ph.D. studying at The <strong>Nankai University (NKU)</strong> under the supervision of <a href="https://cv.nankai.edu.cn/" rel="external nofollow noopener" target="_blank"><b><font color="DarkOrchid">Prof. Jufeng Yang</font></b></a>

                  </td>
                </tr>
              </table>
            </div>
          </div>

          <!-- Latest posts -->
          

          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2>
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/2023_WECL.jpg"></div>

        <!-- Entry bib key -->
        <div id="Zhang_2023_CVPR" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network</div>
          <!-- Author -->
          <div class="author">
          

          <em>Zhicheng Zhang</em>, Lijuan Wang, and Jufeng Yang</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://cvpr2023.thecvf.com/virtual/2023/poster/23010" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
            <a href="/assets/pdf/2023_WECL.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/nku-zhichengzhang/WECL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Automatically predicting the emotions of user-generated videos (UGVs) receives increasing interest recently. However, existing methods mainly focus on a few key visual frames, which may limit their capacity to encode the context that depicts the intended emotions. To tackle that, in this paper, we propose a cross-modal temporal erasing network that locates not only keyframes but also context and audio-related information in a weakly-supervised manner. In specific, we first leverage the intra- and inter-modal relationship among different segments to accurately select keyframes. Then, we iteratively erase keyframes to encourage the model to concentrate on the contexts that include complementary information. Extensive experiments on three challenging benchmark datasets demonstrate that the proposed method performs favorably against the state-of-the-art approaches.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Zhang_2023_CVPR</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zhicheng and Wang, Lijuan and Yang, Jufeng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/2023_PlaneSeg.jpg"></div>

        <!-- Entry bib key -->
        <div id="10097456" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">PlaneSeg: Building a Plug-In for Boosting Planar Region Segmentation</div>
          <!-- Author -->
          <div class="author">
          

          <em>Zhicheng Zhang</em>, Song Chen, Zichuan Wang, and
            <span class="more-authors" title="click to view 1 more author" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '1 more author' ? 'Jufeng Yang' : '1 more author';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '1');
                ">1 more author</span>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>IEEE Transactions on Neural Networks and Learning Systems</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/document/10097456/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
            <a href="/assets/pdf/2023_PlaneSeg.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/nku-zhichengzhang/PlaneSeg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://dx.doi.org/10.1109/TNNLS.2023.3262544" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI: 10.1109/TNNLS.2023.3262544</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Existing methods in planar region segmentation suffer the problems of vague boundaries and failure to detect small-sized regions. To address these, this study presents an end-to-end framework, named PlaneSeg, which can be easily integrated into various plane segmentation models. Specifically, PlaneSeg contains three modules, namely the edge feature extraction module, the multi-scale module, and the resolution-adaptation module. First, the edge feature extraction module produces edge-aware feature maps for finer segmentation boundaries. The learned edge information acts as a constraint to mitigate inaccurate boundaries. Second, the multi-scale module combines feature maps of different layers to harvest spatial and semantic information from planar objects. The multiformity of object information can help recognize small-sized objects to produce more accurate segmentation results. Third, the resolution-adaptation module fuses the feature maps produced by the two aforementioned modules. For this module, a pair-wise feature fusion is adopted to resample the dropped pixels and extract more detailed features. Extensive experiments demonstrate that PlaneSeg outperforms other state-of-the-art approaches on three downstream tasks, including plane segmentation, 3D plane reconstruction, and depth prediction.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10097456</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zhicheng and Chen, Song and Wang, Zichuan and Yang, Jufeng}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Neural Networks and Learning Systems}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PlaneSeg: Building a Plug-In for Boosting Planar Region Segmentation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-15}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TNNLS.2023.3262544}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/2022_TSL300.jpg"></div>

        <!-- Entry bib key -->
        <div id="10.1145/3503161.3548007" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Temporal Sentiment Localization: Listen and Look in Untrimmed Videos</div>
          <!-- Author -->
          <div class="author">
          

          <em>Zhicheng Zhang</em>, and Jufeng Yang</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 30th ACM International Conference on Multimedia</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548007" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
            <a href="/assets/pdf/2022_TSL300.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/nku-zhichengzhang/TSL300" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://dx.doi.org/10.1145/3503161.3548007" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI: 10.1145/3503161.3548007</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Video sentiment analysis aims to uncover the underlying attitudes of viewers, which has a wide range of applications in real world. Existing works simply classify a video into a single sentimental category, ignoring the fact that sentiment in untrimmed videos may appear in multiple segments with varying lengths and unknown locations. To address this, we propose a challenging task, i.e., Temporal Sentiment Localization (TSL), to find which parts of the video convey sentiment. To systematically investigate fully- and weakly-supervised settings for TSL, we first build a benchmark dataset named TSL-300, which is consisting of 300 videos with a total length of 1,291 minutes. Each video is labeled in two ways, one of which is frame-by-frame annotation for the fully-supervised setting, and the other is single-frame annotation, i.e., only a single frame with strong sentiment is labeled per segment for the weakly-supervised setting. Due to the high cost of labeling a densely annotated dataset, we propose TSL-Net in this work, employing single-frame supervision to localize sentiment in videos. In detail, we generate the pseudo labels for unlabeled frames using a greedy search strategy, and fuse the affective features of both visual and audio modalities to predict the temporal sentiment distribution. Here, a reverse mapping strategy is designed for feature fusion, and a contrastive loss is utilized to maintain the consistency between the original feature and the reverse prediction. Extensive experiments show the superiority of our method against the state-of-the-art approaches.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3503161.3548007</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zhicheng and Yang, Jufeng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Temporal Sentiment Localization: Listen and Look in Untrimmed Videos}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3503161.3548007}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3503161.3548007}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 30th ACM International Conference on Multimedia}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>


          <!-- Social -->
            <div class="social">
              <div class="contact-icons">
                <a href="mailto:%67%6C%6F%72%79%7A%7A%63%36@%73%69%6E%61.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=NcTLUzIAAAAJ#%20your%20Google%20Scholar%20ID" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/nku-zhichengzhang#%20your%20GitHub%20user%20name" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a>
            

              </div>

              <div class="contact-note">
                You can even add a little note about which of these is the best way to reach you.

              </div>

            </div>
        </article>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Zhicheng  Zhang(张知诚). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

  </body>
</html>
