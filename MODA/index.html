<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Homepage of MODA"/>
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MODA</title>
  <link rel="icon" type="image/x-icon" href="/assets/img/moda/logo1.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="/assets/css/bulma.min.css">
  <link rel="stylesheet" href="/assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="/assets/css/bulma-slider.min.css">
  <link rel="stylesheet" href="/assets/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/mpot.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="/assets/js/fontawesome.all.min.js"></script>
  <script src="/assets/js/bulma-carousel.min.js"></script>
  <script src="/assets/js/bulma-slider.min.js"></script>
  <script src="/assets/js/mpot.js"></script>

  <style>
    body {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      margin: 0;
      background-color: #f4f4f4;
    }

    .moda {
      font-family: 'Castoro', serif;
      font-weight: bold;
      font-size: 100px;
      background: linear-gradient(to right, #003366, #018284);
      -webkit-background-clip: text;
      color: transparent;
      display: inline-block;
    }
  </style>
</head>
<body>
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://zzcheng.top">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://zzcheng.top/MODA">
            MODA - ICML 2025 Spotlight
          </a>
          <a class="navbar-item" href="https://zzcheng.top/ExtDM">
            ExtDM - CVPR 2024
          </a>
          <a class="navbar-item" href="https://zzcheng.top/MART">
            MART - CVPR 2024
          </a>
          <a class="navbar-item" href="https://zzcheng.top/MPOT">
            MPOT - ICCV 2023
          </a>
          <a class="navbar-item" href="https://zzcheng.top/CTEN">
            CTEN - CVPR 2023
          </a>
          <a class="navbar-item" href="https://zzcheng.top/PlaneSeg">
            PlaneSeg - TNNLS 2023
          </a>
          <a class="navbar-item" href="https://zzcheng.top/tsl300">
            TSL300 - ACMMM 2022
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- Abbreviation -->
          <h1 class="title is-1 publication-title"><img src="/assets/img/moda/logo1.png" style="vertical-align: sub;" width="160">&nbsp;</h1>

          <!-- Title -->
          <h1 class="title is-1 publication-title"><div class="moda">MODA</div>: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding</h1>

          <!-- Publication -->
          <div class="column is-full_width">
            <h2 class="title is-4">ICML 2025 Spotlight</h2>
          </div>

          <!-- Paper authors -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zzcheng.top/" target="_blank">Zhicheng Zhang</a><sup>1,2,†</sup>,
            </span>
            <span class="author-block">
              Wuyou Xia<sup>1</sup>,
            </span>
            <span class="author-block">
              Cheni Zhao<sup>1,†</sup>,
            </span>
            <span class="author-block">
              Yan Zhou<sup>3</sup>,
            </span>
            <span class="author-block">
              Xiaoqiang Liu<sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://yongjie-zhu.github.io/" target="_blank">Yongjie Zhu</a><sup>3,‡</sup>,
            </span>
            <span class="author-block">
              Wenyu Qin<sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=P6MraaYAAAAJ&hl=en/" target="_blank">Pengfei Wan</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Di Zhang<sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://cv.nankai.edu.cn/" target="_blank">Jufeng Yang</a><sup>1,2,✉</sup>
            </span>
          </div>

          <!-- Institution -->
          <div class="is-size-5 publication-authors">
            <sup>1</sup><span class="author-block">Nankai University</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <sup>2</sup><span class="author-block">Pengcheng Laboratory</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <sup>3</sup><span class="author-block">Kuaishou Technology</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="is-size-5 publication-authors">
            <sup>†</sup><span class="author-block">Work done at KlingAI</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <sup>‡</sup><span class="author-block">Project Leader</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <sup>✉</sup><span class="author-block">Corresponding Author</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <!-- Links -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF link -->
              <span class="link-block">
                <a href="/assets/pdf/2025_ICML_MODA.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="/assets/pdf/2025_ICML_MODA.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="/assets/icons/arxiv-logomark-small.svg" alt="arXiv Icon" style="height: 24px; width: 24px;">
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/KwaiVGI/MODA" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/KwaiVGI/MODA" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="/assets/icons/hf-logo.svg" alt="Hugging Face Icon" style="height: 24px; width: 24px;">
                  </span>
                  <span>MODA</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/KwaiVGI/MODA-Emo/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="/assets/icons/hf-logo.svg" alt="Hugging Face Icon" style="height: 24px; width: 24px;">
                  </span>
                  <span>MODA-Emo</span>
                </a>
              </span>

              <!-- <span class="link-block">
                <a href="/assets/pdf/2024_CVPR_MART_poster.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-image"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span> -->
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser" id="Video">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Explanatory Video</h2>
      <figure>
        <video autoplay controls loop height="100%">
          <source src="/assets/video/mpot/iccv23_demo.mp4">
        </video>
      </figure>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            <b>TL;DR: We i) identify attention deficit disorder as a critical barrier hindering fine-grained content understanding in MLLMs; ii) introduce a modular duplex attention mechanism to mitigate modality bias and enhance attention score justification; and iii) develop MODA-based MLLMs that enable fine-grained multimodal understanding across perception, cognition, and emotion tasks.
            </b>
          </p>
          <p>
            Multimodal large language models (MLLMs) recently showed strong capacity in integrating data among multiple modalities, empowered by a generalizable attention architecture. Advanced methods predominantly focus on language-centric tuning while less exploring multimodal tokens mixed through attention, posing challenges in high-level tasks that require fine-grained cognition and emotion understanding. In this work, we identify the attention deficit disorder problem in multimodal learning, caused by inconsistent cross-modal attention and layer-by-layer decayed attention activation. To address this, we propose a novel attention mechanism, termed MOdular Duplex Attention (\model), simultaneously conducting the inner-modal refinement and inter-modal interaction. MODA employs a correct-after-align strategy to effectively decouple modality alignment from cross-layer token mixing. In the alignment phase, tokens are mapped to duplex modality spaces based on the basis vectors, enabling the interaction between visual and language modality. Further, the correctness of attention scores is ensured through adaptive masked attention, which enhances the model's flexibility by allowing customizable masking patterns for different modalities. Extensive experiments on 21 benchmark datasets verify the effectiveness of MODA in perception, cognition, and emotion tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">

            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->

<!-- <section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">PRTrack: Tracking-by-Reasoning Framework</h2>
        <div class="content has-text-justified">
          <img src="/assets/img/mpot/pipeline.png" >
          <p>
            <strong>How to track multiple planar objects against occlusion?</strong>
            <br>
            1. <em>Memory Pool module</em>. Given a planar target, we reformulate the problem of tracking as predicting its mask and the ordered vertexes. The high-dimensional mask can accurately model the change of targets.
            <br>
            2. <em>Appearance Perception</em>. We use a dual-branch network to predict based on the historical tracking results. The masks are aggregated with a multi-layered layout, i.e., a stack of occluders and occludees.
            <br>
            3. <em>Occlusion Reasoning</em>. To solve the cases of complex occlusion, we indicate the occluded part by trajectory, which is predicted by factorized homography matrix, and then fuse the occluded area with the coarse results.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- Video carousel -->
<!-- End video carousel -->





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{zhang2025MODA,
  title={MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding},
  author={Zhicheng Zhang and Wuyou Xia and Cheni Zhao and Zhou Yan and Xiaoqiang Liu and Yongjie Zhu and Wenyu Qin and Pengfei Wan and Di ZHANG and Jufeng Yang},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">this</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
</body>
</html>